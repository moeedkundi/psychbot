# Psychbot Environment Configuration
# Copy this file to .env and configure your settings

# ======================
# OLLAMA CONFIGURATION
# ======================

# Ollama API URL (default: http://localhost:11434)
OLLAMA_API_URL=http://localhost:11434

# Model name to use for interviews
OLLAMA_MODEL=llama3.2:3b

# Request timeout in seconds (default: 30)
OLLAMA_TIMEOUT=300

# ======================
# GENERATION PARAMETERS
# ======================

# Temperature controls randomness (0.0 = deterministic, 2.0 = very random)
OLLAMA_TEMPERATURE=0.7

# Maximum number of tokens to generate
OLLAMA_MAX_TOKENS=2048

# Top-p sampling (0.0 to 1.0)
OLLAMA_TOP_P=0.9

# Top-k sampling (number of top tokens to consider)
OLLAMA_TOP_K=40

# Repeat penalty (1.0 = no penalty, > 1.0 = penalize repetition)
OLLAMA_REPEAT_PENALTY=1.1

# System prompt for the assistant
OLLAMA_SYSTEM_PROMPT="You are a helpful AI assistant powered by Mistral. Provide thoughtful, accurate, and helpful responses."

# Enable streaming responses (true/false)
OLLAMA_STREAM=false

# ======================
# CHROMADB CONFIGURATION
# ======================

# Path to ChromaDB vector database
CHROMA_DB_PATH=./data/vector_db

# Collection name for interview knowledge
CHROMA_COLLECTION_NAME=interview_knowledge

# Sentence transformer model for embeddings
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Device for embeddings (cpu/cuda)
EMBEDDING_DEVICE=cpu

# ======================
# FASTAPI CONFIGURATION
# ======================

# API server settings
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# CORS settings for frontend integration
CORS_ORIGINS=["http://localhost:3000", "http://localhost:8080"]

# ======================
# APPLICATION SETTINGS
# ======================

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Log format
LOG_FORMAT="%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Output format (rich, json, plain)
OUTPUT_FORMAT=rich

# Maximum conversation history to keep (number of exchanges)
MAX_HISTORY=10

# Directory for generated reports
REPORTS_DIR=./reports

# Include performance charts in reports
INCLUDE_CHARTS=true

# ======================
# MCP CONFIGURATION
# ======================

# Enable MCP (Model Context Protocol) integration
MCP_ENABLED=false

# Path to MCP server executable
MCP_SERVER_PATH=

# ======================
# EXAMPLES OF OTHER MODELS
# ======================

# For Llama 2 7B:
# OLLAMA_MODEL_NAME=llama2:7b

# For Code Llama:
# OLLAMA_MODEL_NAME=codellama:7b

# For DeepSeek R1:
# OLLAMA_MODEL_NAME=deepseek-r1:8b

# For higher creativity:
# OLLAMA_TEMPERATURE=1.2
# OLLAMA_TOP_P=0.95

# For more focused responses:
# OLLAMA_TEMPERATURE=0.3
# OLLAMA_TOP_P=0.8
# OLLAMA_TOP_K=20